{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bb892c",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8001a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 3969\n",
      "Test size 997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emergency Room Reports</td>\n",
       "      <td>REASON FOR THE VISIT:,  Very high PT/INR.,HIST...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>NAME OF PROCEDURE,1.  Selective coronary angio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         medical_specialty                                      transcription  \\\n",
       "0   Emergency Room Reports  REASON FOR THE VISIT:,  Very high PT/INR.,HIST...   \n",
       "1                  Surgery  PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...   \n",
       "2                  Surgery  NAME OF PROCEDURE,1.  Selective coronary angio...   \n",
       "\n",
       "   labels  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"new_test.csv\", index_col=0)\n",
    "\n",
    "print(\"Train size\", len(train_df))\n",
    "print(\"Test size\", len(test_df))\n",
    "train_df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9ceed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3969 entries, 0 to 3999\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   medical_specialty  3969 non-null   object\n",
      " 1   transcription      3969 non-null   object\n",
      " 2   labels             3969 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 124.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 997 entries, 0 to 996\n",
      "Data columns (total 1 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   transcription  997 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffbcd6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transcription    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326d130",
   "metadata": {},
   "source": [
    "### Train Set Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c349df00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Surgery                          863\n",
       " Consult - History and Phy.       410\n",
       " Cardiovascular / Pulmonary       309\n",
       " Orthopedic                       289\n",
       " Radiology                        213\n",
       " General Medicine                 209\n",
       " Gastroenterology                 176\n",
       " Neurology                        170\n",
       " SOAP / Chart / Progress Notes    135\n",
       " Urology                          134\n",
       " Obstetrics / Gynecology          123\n",
       " Discharge Summary                 87\n",
       " ENT - Otolaryngology              82\n",
       " Neurosurgery                      71\n",
       " Hematology - Oncology             68\n",
       " Ophthalmology                     67\n",
       " Emergency Room Reports            63\n",
       " Nephrology                        63\n",
       " Pediatrics - Neonatal             55\n",
       " Pain Management                   54\n",
       " Psychiatry / Psychology           45\n",
       " Office Notes                      38\n",
       " Podiatry                          35\n",
       " Dermatology                       21\n",
       " Dentistry                         21\n",
       " Cosmetic / Plastic Surgery        19\n",
       " Letters                           19\n",
       " Endocrinology                     16\n",
       " Physical Medicine - Rehab         16\n",
       " Bariatrics                        15\n",
       " IME-QME-Work Comp etc.            12\n",
       " Chiropractic                      12\n",
       " Sleep Medicine                    12\n",
       " Diets and Nutritions               9\n",
       " Speech - Language                  8\n",
       " Autopsy                            7\n",
       " Hospice - Palliative Care          6\n",
       " Allergy / Immunology               6\n",
       " Rheumatology                       6\n",
       " Lab Medicine - Pathology           5\n",
       "Name: medical_specialty, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"medical_specialty\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8c8f3",
   "metadata": {},
   "source": [
    "### Sample Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b4b315a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('REASON FOR THE VISIT:,  Very high PT/INR.,HISTORY: , The patient is an '\n",
      " '81-year-old lady whom I met last month when she came in with pneumonia and '\n",
      " 'CHF.  She was noticed to be in atrial fibrillation, which is a chronic '\n",
      " 'problem for her.  She did not want to have Coumadin started because she said '\n",
      " 'that she has had it before and the INR has had been very difficult to '\n",
      " 'regulate to the point that it was dangerous, but I convinced her to restart '\n",
      " 'the Coumadin again.  I gave her the Coumadin as an outpatient and then the '\n",
      " 'INR was found to be 12.  So, I told her to come to the emergency room to get '\n",
      " 'vitamin K to reverse the anticoagulation.,PAST MEDICAL HISTORY:,1.  '\n",
      " 'Congestive heart failure.,2.  Renal insufficiency.,3.  Coronary artery '\n",
      " 'disease.,4.  Atrial fibrillation.,5.  COPD.,6.  Recent pneumonia.,7.  '\n",
      " 'Bladder cancer.,8.  History of ruptured colon.,9.  Myocardial '\n",
      " 'infarction.,10.  Hernia repair.,11.  Colon resection.,12.  Carpal tunnel '\n",
      " 'repair.,13.  Knee surgery.,MEDICATIONS:,1.  Coumadin.,2.  Simvastatin.,3.  '\n",
      " 'Nitrofurantoin.,4.  Celebrex.,5.  Digoxin.,6.  Levothyroxine.,7.  '\n",
      " 'Vicodin.,8.  Triamterene and hydrochlorothiazide.,9.  Carvedilol.,SOCIAL '\n",
      " 'HISTORY:  ,She does not smoke and she does not drink.,PHYSICAL '\n",
      " 'EXAMINATION:,GENERAL:  Lady in no distress.,VITAL SIGNS:  Blood pressure '\n",
      " '100/46, pulse of 75, respirations 12, and temperature 98.2.,HEENT:  Head is '\n",
      " 'normal.,NECK:  Supple.,LUNGS:  Clear to auscultation and percussion.,HEART:  '\n",
      " 'No S3, no S4, and no murmurs.,ABDOMEN:  Soft.,EXTREMITIES:  Lower '\n",
      " 'extremities, no edema.,ASSESSMENT:,1.  Atrial fibrillation.,2.  '\n",
      " 'Coagulopathy, induced by Coumadin.,PLAN: , Her INR at the office was 12.  I '\n",
      " 'will repeat it, and if it is still elevated, I will give vitamin K 10 mg in '\n",
      " '100 mL of D5W and then send her home and repeat the PT/INR next week.  I '\n",
      " 'believe at this time that it is too risky to use Coumadin in her case '\n",
      " 'because of her age and comorbidities, the multiple medications that she '\n",
      " 'takes and it is very difficult to keep an adequate level of anticoagulation '\n",
      " 'that is safe for her.  She is prone to a fall and this would be a big '\n",
      " 'problem.  We will use one aspirin a day instead of the anticoagulation.  She '\n",
      " 'is aware of the risk of stroke, but she is very scared of the '\n",
      " 'anticoagulation with Coumadin and does not want to use the Coumadin at this '\n",
      " 'time and I understand.  We will see her as an outpatient.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(train_df.transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8cf75",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14302e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>Dentistry</td>\n",
       "      <td>preoperative diagnosis bilateral open mandible...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>preoperative diagnosis  adenocarcinoma of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>Obstetrics / Gynecology</td>\n",
       "      <td>preoperative diagnosis complex right lower qua...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>Hematology - Oncology</td>\n",
       "      <td>reason for consultation i was asked by dr  x t...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>ENT - Otolaryngology</td>\n",
       "      <td>procedures performed   functional endoscopic s...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             medical_specialty  \\\n",
       "1931                 Dentistry   \n",
       "2152                   Surgery   \n",
       "1738   Obstetrics / Gynecology   \n",
       "3944     Hematology - Oncology   \n",
       "401       ENT - Otolaryngology   \n",
       "\n",
       "                                          transcription  labels  \n",
       "1931  preoperative diagnosis bilateral open mandible...      29  \n",
       "2152  preoperative diagnosis  adenocarcinoma of the ...       1  \n",
       "1738  preoperative diagnosis complex right lower qua...      19  \n",
       "3944  reason for consultation i was asked by dr  x t...      11  \n",
       "401   procedures performed   functional endoscopic s...       9  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.transcription = train_df.transcription.astype('str')\n",
    "train_df.transcription = train_df.transcription.str.lower()\n",
    "\n",
    "train_df = train_df.dropna(axis = 0, how ='any')\n",
    "\n",
    "#getting rid of targeted charachters in the trascription\n",
    "chars = ['#',':,',': ,',';','$','!','?','*','``','%', '1', '2', '3', '4', '5','6','7','8','9','10']\n",
    "for c in chars:\n",
    "    train_df.transcription = train_df.transcription.str.replace(c,\"\")\n",
    "\n",
    "train_df.sample(5)\n",
    "\n",
    "#getting rid of targeted charachters in the trascription\n",
    "chars = [\",\", \".\", \"[\", \"]\", \":\", \"``\", \")\", \"(\"]\n",
    "for c in chars:\n",
    "    train_df.transcription = train_df.transcription.str.replace(c,\" \")\n",
    "\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f27e84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pjyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pjyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/pjyi/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>Discharge Summary</td>\n",
       "      <td>diagnosis chronic laryngitis  hoarseness  hist...</td>\n",
       "      <td>21</td>\n",
       "      <td>[diagnosis, chronic, laryngitis, hoarseness, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Pediatrics - Neonatal</td>\n",
       "      <td>admitting diagnoses   respiratory distress    ...</td>\n",
       "      <td>30</td>\n",
       "      <td>[admitting, diagnoses, respiratory, distress, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>preoperative diagnosis fractured right fifth m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[preoperative, diagnosis, fractured, right, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>operative procedure   redo coronary bypass gra...</td>\n",
       "      <td>7</td>\n",
       "      <td>[operative, procedure, redo, coronary, bypass,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>SOAP / Chart / Progress Notes</td>\n",
       "      <td>subjective this patient presents to the office...</td>\n",
       "      <td>13</td>\n",
       "      <td>[subjective, this, patient, presents, to, the,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   medical_specialty  \\\n",
       "1344               Discharge Summary   \n",
       "405            Pediatrics - Neonatal   \n",
       "3319                         Surgery   \n",
       "2225      Cardiovascular / Pulmonary   \n",
       "2474   SOAP / Chart / Progress Notes   \n",
       "\n",
       "                                          transcription  labels  \\\n",
       "1344  diagnosis chronic laryngitis  hoarseness  hist...      21   \n",
       "405   admitting diagnoses   respiratory distress    ...      30   \n",
       "3319  preoperative diagnosis fractured right fifth m...       1   \n",
       "2225  operative procedure   redo coronary bypass gra...       7   \n",
       "2474  subjective this patient presents to the office...      13   \n",
       "\n",
       "                                              tokenized  \n",
       "1344  [diagnosis, chronic, laryngitis, hoarseness, h...  \n",
       "405   [admitting, diagnoses, respiratory, distress, ...  \n",
       "3319  [preoperative, diagnosis, fractured, right, fi...  \n",
       "2225  [operative, procedure, redo, coronary, bypass,...  \n",
       "2474  [subjective, this, patient, presents, to, the,...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['tokenized'] = train_df.transcription.apply(nltk.word_tokenize)\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47ab0d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reason', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('visit', 'NN'),\n",
       " ('very', 'RB'),\n",
       " ('high', 'JJ'),\n",
       " ('pt/inr', 'NN'),\n",
       " ('history', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('patient', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('-year-old', 'JJ'),\n",
       " ('lady', 'NN'),\n",
       " ('whom', 'WP'),\n",
       " ('i', 'VBZ'),\n",
       " ('met', 'VBD'),\n",
       " ('last', 'JJ'),\n",
       " ('month', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('she', 'PRP'),\n",
       " ('came', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('with', 'IN'),\n",
       " ('pneumonia', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('chf', 'NN'),\n",
       " ('she', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('noticed', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('in', 'IN'),\n",
       " ('atrial', 'JJ'),\n",
       " ('fibrillation', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('chronic', 'JJ'),\n",
       " ('problem', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('she', 'PRP'),\n",
       " ('did', 'VBD'),\n",
       " ('not', 'RB'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('have', 'VB'),\n",
       " ('coumadin', 'NN'),\n",
       " ('started', 'VBN'),\n",
       " ('because', 'IN'),\n",
       " ('she', 'PRP'),\n",
       " ('said', 'VBD'),\n",
       " ('that', 'IN'),\n",
       " ('she', 'PRP'),\n",
       " ('has', 'VBZ'),\n",
       " ('had', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('before', 'IN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('inr', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('had', 'VBN'),\n",
       " ('been', 'VBN'),\n",
       " ('very', 'RB'),\n",
       " ('difficult', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('regulate', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('point', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('dangerous', 'JJ'),\n",
       " ('but', 'CC'),\n",
       " ('i', 'JJ'),\n",
       " ('convinced', 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('restart', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('coumadin', 'NN'),\n",
       " ('again', 'RB'),\n",
       " ('i', 'JJ'),\n",
       " ('gave', 'VBD'),\n",
       " ('her', 'PRP$'),\n",
       " ('the', 'DT'),\n",
       " ('coumadin', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('outpatient', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('then', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('inr', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('found', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('so', 'RB'),\n",
       " ('i', 'JJ'),\n",
       " ('told', 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('come', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('emergency', 'NN'),\n",
       " ('room', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('vitamin', 'JJ'),\n",
       " ('k', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('reverse', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('anticoagulation', 'NN'),\n",
       " ('past', 'IN'),\n",
       " ('medical', 'JJ'),\n",
       " ('history', 'NN'),\n",
       " ('congestive', 'JJ'),\n",
       " ('heart', 'NN'),\n",
       " ('failure', 'NN'),\n",
       " ('renal', 'JJ'),\n",
       " ('insufficiency', 'NN'),\n",
       " ('coronary', 'JJ'),\n",
       " ('artery', 'NN'),\n",
       " ('disease', 'NN'),\n",
       " ('atrial', 'JJ'),\n",
       " ('fibrillation', 'NN'),\n",
       " ('copd', 'NN'),\n",
       " ('recent', 'JJ'),\n",
       " ('pneumonia', 'NN'),\n",
       " ('bladder', 'NN'),\n",
       " ('cancer', 'NN'),\n",
       " ('history', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('ruptured', 'JJ'),\n",
       " ('colon', 'NN'),\n",
       " ('myocardial', 'JJ'),\n",
       " ('infarction', 'NN'),\n",
       " ('0', 'CD'),\n",
       " ('hernia', 'NN'),\n",
       " ('repair', 'NN'),\n",
       " ('colon', 'NN'),\n",
       " ('resection', 'NN'),\n",
       " ('carpal', 'JJ'),\n",
       " ('tunnel', 'NN'),\n",
       " ('repair', 'NN'),\n",
       " ('knee', 'NN'),\n",
       " ('surgery', 'NN'),\n",
       " ('medications', 'NNS'),\n",
       " ('coumadin', 'VBP'),\n",
       " ('simvastatin', 'JJ'),\n",
       " ('nitrofurantoin', 'JJ'),\n",
       " ('celebrex', 'NN'),\n",
       " ('digoxin', 'NN'),\n",
       " ('levothyroxine', 'NN'),\n",
       " ('vicodin', 'NN'),\n",
       " ('triamterene', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('hydrochlorothiazide', 'VB'),\n",
       " ('carvedilol', 'JJ'),\n",
       " ('social', 'JJ'),\n",
       " ('history', 'NN'),\n",
       " ('she', 'PRP'),\n",
       " ('does', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('smoke', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('she', 'PRP'),\n",
       " ('does', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('physical', 'JJ'),\n",
       " ('examinationgeneral', 'JJ'),\n",
       " ('lady', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('no', 'DT'),\n",
       " ('distress', 'NN'),\n",
       " ('vital', 'JJ'),\n",
       " ('signs', 'NNS'),\n",
       " ('blood', 'NN'),\n",
       " ('pressure', 'NN'),\n",
       " ('00/', 'CD'),\n",
       " ('pulse', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('respirations', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('temperature', 'NN'),\n",
       " ('heent', 'NN'),\n",
       " ('head', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('normal', 'JJ'),\n",
       " ('neck', 'RB'),\n",
       " ('supple', 'JJ'),\n",
       " ('lungs', 'NNS'),\n",
       " ('clear', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('auscultation', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('percussion', 'NN'),\n",
       " ('heart', 'NN'),\n",
       " ('no', 'DT'),\n",
       " ('s', 'NN'),\n",
       " ('no', 'DT'),\n",
       " ('s', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('no', 'DT'),\n",
       " ('murmurs', 'NNS'),\n",
       " ('abdomen', 'VBP'),\n",
       " ('soft', 'JJ'),\n",
       " ('extremities', 'NNS'),\n",
       " ('lower', 'JJR'),\n",
       " ('extremities', 'VBZ'),\n",
       " ('no', 'DT'),\n",
       " ('edema', 'JJ'),\n",
       " ('assessment', 'NN'),\n",
       " ('atrial', 'JJ'),\n",
       " ('fibrillation', 'NN'),\n",
       " ('coagulopathy', 'NN'),\n",
       " ('induced', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('coumadin', 'NN'),\n",
       " ('plan', 'NN'),\n",
       " ('her', 'PRP$'),\n",
       " ('inr', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('office', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('i', 'VBN'),\n",
       " ('will', 'MD'),\n",
       " ('repeat', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('if', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('still', 'RB'),\n",
       " ('elevated', 'VBN'),\n",
       " ('i', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('give', 'VB'),\n",
       " ('vitamin', 'NN'),\n",
       " ('k', 'NN'),\n",
       " ('0', 'CD'),\n",
       " ('mg', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('00', 'CD'),\n",
       " ('ml', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('dw', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('then', 'RB'),\n",
       " ('send', 'VB'),\n",
       " ('her', 'PRP$'),\n",
       " ('home', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('repeat', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('pt/inr', 'NN'),\n",
       " ('next', 'IN'),\n",
       " ('week', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('believe', 'VBP'),\n",
       " ('at', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('too', 'RB'),\n",
       " ('risky', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('use', 'VB'),\n",
       " ('coumadin', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('case', 'NN'),\n",
       " ('because', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('age', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('comorbidities', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('multiple', 'JJ'),\n",
       " ('medications', 'NNS'),\n",
       " ('that', 'IN'),\n",
       " ('she', 'PRP'),\n",
       " ('takes', 'VBZ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('difficult', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('keep', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('adequate', 'JJ'),\n",
       " ('level', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('anticoagulation', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('safe', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('she', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('prone', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('a', 'DT'),\n",
       " ('fall', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('this', 'DT'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('problem', 'NN'),\n",
       " ('we', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('use', 'VB'),\n",
       " ('one', 'CD'),\n",
       " ('aspirin', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('day', 'NN'),\n",
       " ('instead', 'RB'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('anticoagulation', 'NN'),\n",
       " ('she', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('aware', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('risk', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('stroke', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('she', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('scared', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('anticoagulation', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('coumadin', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('does', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('use', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('coumadin', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('i', 'JJ'),\n",
       " ('understand', 'VBP'),\n",
       " ('we', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('her', 'PRP'),\n",
       " ('as', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('outpatient', 'NN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos_tag\n",
    "from nltk import pos_tag\n",
    "nltk.tag.pos_tag(train_df['tokenized'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9994253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(reason, NN), (for, IN), (the, DT), (visit, N...\n",
       "1    [(preoperative, JJ), (diagnosis, NN), (acetabu...\n",
       "2    [(name, NN), (of, IN), (procedure, NN), (selec...\n",
       "3    [(referring, VBG), (diagnosis, NN), (motor, NN...\n",
       "4    [(chief, JJ), (complaint, NN), (dental, NN), (...\n",
       "Name: POSTags, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['POSTags'] = train_df['tokenized'].apply(pos_tag)\n",
    "train_df['POSTags'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34848512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(reason, NN), (visit, NN), (pt/inr, NN), (his...\n",
       "1       [(diagnosis, NN), (fracture, NN), (left, NN), ...\n",
       "2       [(name, NN), (procedure, NN), (angiography, NN...\n",
       "3       [(referring, VBG), (diagnosis, NN), (motor, NN...\n",
       "4       [(complaint, NN), (dental, NN), (pain, NN), (h...\n",
       "                              ...                        \n",
       "3995    [(problems, NNS), (issues, NNS), (headaches, N...\n",
       "3996    [(diagnosis, NN), (anemia, NN), (procedure, NN...\n",
       "3997    [(dysphagia, NN), (gastroesophageal, NN), (ref...\n",
       "3998    [(patient, NN), (abdomen, NNS), (was, VBD), (p...\n",
       "3999    [(diagnosis, NN), (effusion, NN), (failure, NN...\n",
       "Name: Nouns, Length: 3969, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the nouns in our corpus\n",
    "# train_df['Nouns'] = train_df['POSTags'].apply(lambda x: [(t[0], t[1]) for t in x if t[1]=='NN' or t[1]=='NNP' or t[1]=='NNS' or t[1]=='NNPS' or t[1]=='VB' or t[1]=='VBN' or t[1]=='VBD' or t[1]=='VBZ'])\n",
    "\n",
    "train_df['Nouns'] = train_df['POSTags'].apply(lambda x: [(t[0], t[1]) for t in x if t[1].startswith(('NN', 'VB'))])\n",
    "\n",
    "train_df['Nouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fe518d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [reason, visit, pt/inr, history, patient, is, ...\n",
       "1       [diagnosis, fracture, left, column/transverse,...\n",
       "2       [name, procedure, angiography, placement, over...\n",
       "3       [referring, diagnosis, motor, neuron, disease,...\n",
       "4       [complaint, dental, pain, history, illness, is...\n",
       "                              ...                        \n",
       "3995    [problems, issues, headaches, consistent, diag...\n",
       "3996    [diagnosis, anemia, procedure, endoscopy, diag...\n",
       "3997    [dysphagia, gastroesophageal, reflux, disease,...\n",
       "3998    [patient, abdomen, was, prepped, draped, fashi...\n",
       "3999    [diagnosis, effusion, failure, dyspnea, diagno...\n",
       "Name: filtered_transcription, Length: 3969, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['filtered_transcription'] = [[t[0] for t in row] for row in train_df['Nouns']]\n",
    "\n",
    "train_df['filtered_transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61acdeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [reason, visit, pt/inr, history, patient, lady...\n",
       "1       [diagnosis, fracture, left, column/transverse,...\n",
       "2       [name, procedure, angiography, placement, over...\n",
       "3       [referring, diagnosis, motor, neuron, disease,...\n",
       "4       [complaint, dental, pain, history, illness, fe...\n",
       "                              ...                        \n",
       "3995    [problems, issues, headaches, consistent, diag...\n",
       "3996    [diagnosis, anemia, procedure, endoscopy, diag...\n",
       "3997    [dysphagia, gastroesophageal, reflux, disease,...\n",
       "3998    [patient, abdomen, prepped, draped, fashion, s...\n",
       "3999    [diagnosis, effusion, failure, dyspnea, diagno...\n",
       "Name: filtered_transcription, Length: 3969, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_df['filtered_transcription'] = [[w for w in row if not w in stop_words] for row in train_df['filtered_transcription']]\n",
    "\n",
    "train_df['filtered_transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb9698b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "dictionary = train_df['filtered_transcription'].transform(lambda x: Counter(x)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdd2c670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17991\n",
      "273\n",
      "['patient', 'history', 'procedure', 'placed', 'left', 'pain', 'using', 'noted', 'time', 'performed', 'blood', 'removed', 'incision', 'used', 'room', 'mg', 'diagnosis', 'artery', 'skin', 'taken', 'disease', 'position', 'area', 'anesthesia', 'x', 'fashion', 'made', 'pressure', '-0', 'medications', 'obtained', 'closed', 'abdomen', 'operating', 'heart', 'neck', 'surgery', 'examination', 'evidence', 'given', 'day', 'tissue', 'condition', 'catheter', 'denies', 'done', 'suture', 'seen', 'care', 'loss', 'p', 'complications', 'side', 'extremities', 'family', 'symptoms', 'draped', 'vicryl', 'prepped', 'cm', 'findings', 'level', 'dr', 'showed', 'head', 'right', 'rate', 'identified', 'wound', 'chest', 'signs', 'bleeding', 'diagnoses', 'brought', 'bladder', 'plan', 'none', 'dissection', 'found', 'bone', 'allergies', 'muscle', 'exam', 'hospital', 'years', 'status', 'mass', 'tolerated', 'discussed', 'female', 'point', 'mm', 'applied', 'today', 'difficulty', 'hemostasis', 'evaluation', 'bowel', 'treatment', 'days', 'use', 'nerve', 'discharge', 'review', 'fascia', 'tube', 'home', 'states', 'stenosis', 'recovery', 'extremity', 'knee', 'sutures', 'weeks', 'vein', 'revealed', 'place', 'problems', 'eye', '/', 'inserted', 'illness', 'risks', 'attention', 'hypertension', 'following', 'aspect', 'see', 'including', 'changes', 'site', 'lesion', 'consent', 'therapy', 'infection', 'foot', 'fracture', 'wall', 'impression', 'systems', 'carried', 'male', 'weight', 'post', 'appeared', 'tumor', 'size', 'minutes', 'lesions', 'irrigated', 'followed', 'estimated', 'emergency', 'ml', 'cancer', 'edema', 'portion', 'space', 'tendon', 'rhythm', 'breath', 'b', 'age', 'need', 'mother', 'body', 'ligament', 'course', 'months', 'sent', 'spine', 'masses', 'diabetes', 'fluid', 'felt', 'motion', 'c', 'region', 'hours', 'weakness', 'cc', 'times', 'operation', 'admitted', 'dissected', 'indications', 'midline', 'placement', 'shows', 'cord', 'elevated', 'sounds', 'l', 'biopsy', 'count', 'motor', 'lobe', 'follow', 'ct', 'continue', 'needle', 'lungs', 'range', 'carotid', 'respiratory', 'injury', 'started', 'week', 'mri', 'technique', 'sheath', 'running', 'end', 'blade', 'detail', 'breast', 'swelling', 'table', 'transferred', 'admission', 'iv', 'appears', 'vomiting', 'distress', 'passed', 'shoulder', 'valve', 'leg', 'shortness', 'test', 'endotracheal', 'injected', 'wire', 'function', 'closure', 'benefits', 'description', 'amount', 'repair', 'drug', 'acute', 'advanced', 'complaint', 'cavity', 'failure', 'brain', 'lumbar', 'alcohol', 'nausea', 'reported', 'risk', 'reviewed', 'tenderness', 'reports', 'arm', 'cut', 'case', 'r', 'colon', 'dorsal', 'system', 'pounds', 'dressing', 'lung', 'length', 'office', 'contrast', 'heent', 'procedures', 'air', 'joint', 'sedation', 'assessment']\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "count = 0\n",
    "removal_array = []\n",
    "for k in sorted(dictionary, key=dictionary.get, reverse=True):\n",
    "    #print(k, dictionary[k])\n",
    "    if int(dictionary[k]) > 500:\n",
    "        count += 1\n",
    "        removal_array.append(k)\n",
    "        \n",
    "print(count)\n",
    "\n",
    "print(removal_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc85b5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [reason, visit, pt/inr, lady, met, month, came...\n",
       "1       [column/transverse, posterior, variety, accomp...\n",
       "2       [name, angiography, overlapping, xience, stent...\n",
       "3       [referring, neuron, briefly, woman, progressio...\n",
       "4       [dental, starting, night, jaw, feel, tongue, t...\n",
       "                              ...                        \n",
       "3995    [issues, headaches, consistent, migraine, reco...\n",
       "3996    [anemia, endoscopy, duodenitis, junction, ulce...\n",
       "3997    [dysphagia, gastroesophageal, reflux, rule, st...\n",
       "3998    [veress, insufflated, trocar, camera, view, de...\n",
       "3999    [effusion, dyspnea, effusion, pleurocentesis, ...\n",
       "Name: filtered_transcription, Length: 3969, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['filtered_transcription'] = [[w for w in row if not w in removal_array] for row in train_df['filtered_transcription']]\n",
    "train_df['filtered_transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37b63875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       reason visit pt/inr lady met month came pneumo...\n",
       "1       column/transverse posterior variety accompanyi...\n",
       "2       name angiography overlapping xience stents aor...\n",
       "3       referring neuron briefly woman progression dys...\n",
       "4       dental starting night jaw feel tongue teeth fr...\n",
       "                              ...                        \n",
       "3995    issues headaches consistent migraine recommend...\n",
       "3996    anemia endoscopy duodenitis junction ulceratio...\n",
       "3997    dysphagia gastroesophageal reflux rule strictu...\n",
       "3998    veress insufflated trocar camera view demonstr...\n",
       "3999    effusion dyspnea effusion pleurocentesis lidoc...\n",
       "Name: transcription, Length: 3969, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['transcription'] = [\" \".join(row) for row in train_df['filtered_transcription']]\n",
    "\n",
    "train_df['transcription']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b6f4d",
   "metadata": {},
   "source": [
    "### Sample Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68977658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install nltk\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e142223",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = train_df[\"medical_specialty\"].unique()\n",
    "\n",
    "# idx_2_class = {i: s for i, s in enumerate(unique_classes)}\n",
    "# class_2_idx = {s: i for i, s in enumerate(unique_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cac5d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"labels\"] = train_df[\"medical_specialty\"].apply(lambda s: class_2_idx[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "233a4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train_df, train_test_df = \\\n",
    "    train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "449f16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = {\n",
    "    'train': Dataset.from_pandas(train_train_df),\n",
    "    'val': Dataset.from_pandas(train_test_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "}\n",
    "\n",
    "ds = DatasetDict(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a995f402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/pjyi/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/pjyi/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/pjyi/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/pjyi/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/pjyi/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb88e31de04b45d9a6b10220f6571701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afb49d67c0c4ec395358d8d4d3b1a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e01362bbf4400ca4c25880ca71fa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_text(texts):\n",
    "    return tokenizer(texts[\"transcription\"], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(tokenize_text, batched=True)\n",
    "ds[\"val\"] = ds[\"val\"].map(tokenize_text, batched=True)\n",
    "ds[\"test\"] = ds[\"test\"].map(tokenize_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa6c4319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/pjyi/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/pjyi/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ca876",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f26fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "414f5205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "logging_steps = len(train_train_df) // batch_size\n",
    "output_dir = \"hf_trainer\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "     num_train_epochs=10,\n",
    "     learning_rate=2e-4,\n",
    "     per_device_train_batch_size=batch_size,\n",
    "     per_device_eval_batch_size=batch_size,\n",
    "     weight_decay=0.01,\n",
    "     evaluation_strategy=\"epoch\",\n",
    "     logging_steps=logging_steps,\n",
    "     push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "994876be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['val'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8cfca94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='870' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [870/870 05:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.703900</td>\n",
       "      <td>2.574905</td>\n",
       "      <td>0.028923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.338100</td>\n",
       "      <td>2.402000</td>\n",
       "      <td>0.042978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.032700</td>\n",
       "      <td>2.197368</td>\n",
       "      <td>0.072925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.809500</td>\n",
       "      <td>2.210747</td>\n",
       "      <td>0.075065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.620100</td>\n",
       "      <td>2.299562</td>\n",
       "      <td>0.086841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.458800</td>\n",
       "      <td>2.250863</td>\n",
       "      <td>0.116425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.314000</td>\n",
       "      <td>2.463608</td>\n",
       "      <td>0.112692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.185000</td>\n",
       "      <td>2.512815</td>\n",
       "      <td>0.107576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.092300</td>\n",
       "      <td>2.662735</td>\n",
       "      <td>0.106656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>2.728560</td>\n",
       "      <td>0.104918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=870, training_loss=1.6434528526218457, metrics={'train_runtime': 344.3989, 'train_samples_per_second': 80.662, 'train_steps_per_second': 2.526, 'total_flos': 1841219072409600.0, 'train_loss': 1.6434528526218457, 'epoch': 10.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6603a",
   "metadata": {},
   "source": [
    "### Making Inference on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = trainer.predict(ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series(pred_y.predictions.argmax(axis=1))\n",
    "a.name = \"Expected\"\n",
    "a.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3b482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708f738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
