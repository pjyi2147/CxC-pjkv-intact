{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bb892c",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8001a2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 3969\n",
      "Test size 997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"new_test.csv\", index_col=0)\n",
    "\n",
    "print(\"Train size\", len(train_df))\n",
    "print(\"Test size\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326d130",
   "metadata": {},
   "source": [
    "### Train Set Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c349df00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Surgery                          863\n",
       " Consult - History and Phy.       410\n",
       " Cardiovascular / Pulmonary       309\n",
       " Orthopedic                       289\n",
       " Radiology                        213\n",
       " General Medicine                 209\n",
       " Gastroenterology                 176\n",
       " Neurology                        170\n",
       " SOAP / Chart / Progress Notes    135\n",
       " Urology                          134\n",
       " Obstetrics / Gynecology          123\n",
       " Discharge Summary                 87\n",
       " ENT - Otolaryngology              82\n",
       " Neurosurgery                      71\n",
       " Hematology - Oncology             68\n",
       " Ophthalmology                     67\n",
       " Emergency Room Reports            63\n",
       " Nephrology                        63\n",
       " Pediatrics - Neonatal             55\n",
       " Pain Management                   54\n",
       " Psychiatry / Psychology           45\n",
       " Office Notes                      38\n",
       " Podiatry                          35\n",
       " Dermatology                       21\n",
       " Dentistry                         21\n",
       " Cosmetic / Plastic Surgery        19\n",
       " Letters                           19\n",
       " Endocrinology                     16\n",
       " Physical Medicine - Rehab         16\n",
       " Bariatrics                        15\n",
       " IME-QME-Work Comp etc.            12\n",
       " Chiropractic                      12\n",
       " Sleep Medicine                    12\n",
       " Diets and Nutritions               9\n",
       " Speech - Language                  8\n",
       " Autopsy                            7\n",
       " Hospice - Palliative Care          6\n",
       " Allergy / Immunology               6\n",
       " Rheumatology                       6\n",
       " Lab Medicine - Pathology           5\n",
       "Name: medical_specialty, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"medical_specialty\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8c8f3",
   "metadata": {},
   "source": [
    "### Sample Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4b315a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('REASON FOR THE VISIT:,  Very high PT/INR.,HISTORY: , The patient is an '\n",
      " '81-year-old lady whom I met last month when she came in with pneumonia and '\n",
      " 'CHF.  She was noticed to be in atrial fibrillation, which is a chronic '\n",
      " 'problem for her.  She did not want to have Coumadin started because she said '\n",
      " 'that she has had it before and the INR has had been very difficult to '\n",
      " 'regulate to the point that it was dangerous, but I convinced her to restart '\n",
      " 'the Coumadin again.  I gave her the Coumadin as an outpatient and then the '\n",
      " 'INR was found to be 12.  So, I told her to come to the emergency room to get '\n",
      " 'vitamin K to reverse the anticoagulation.,PAST MEDICAL HISTORY:,1.  '\n",
      " 'Congestive heart failure.,2.  Renal insufficiency.,3.  Coronary artery '\n",
      " 'disease.,4.  Atrial fibrillation.,5.  COPD.,6.  Recent pneumonia.,7.  '\n",
      " 'Bladder cancer.,8.  History of ruptured colon.,9.  Myocardial '\n",
      " 'infarction.,10.  Hernia repair.,11.  Colon resection.,12.  Carpal tunnel '\n",
      " 'repair.,13.  Knee surgery.,MEDICATIONS:,1.  Coumadin.,2.  Simvastatin.,3.  '\n",
      " 'Nitrofurantoin.,4.  Celebrex.,5.  Digoxin.,6.  Levothyroxine.,7.  '\n",
      " 'Vicodin.,8.  Triamterene and hydrochlorothiazide.,9.  Carvedilol.,SOCIAL '\n",
      " 'HISTORY:  ,She does not smoke and she does not drink.,PHYSICAL '\n",
      " 'EXAMINATION:,GENERAL:  Lady in no distress.,VITAL SIGNS:  Blood pressure '\n",
      " '100/46, pulse of 75, respirations 12, and temperature 98.2.,HEENT:  Head is '\n",
      " 'normal.,NECK:  Supple.,LUNGS:  Clear to auscultation and percussion.,HEART:  '\n",
      " 'No S3, no S4, and no murmurs.,ABDOMEN:  Soft.,EXTREMITIES:  Lower '\n",
      " 'extremities, no edema.,ASSESSMENT:,1.  Atrial fibrillation.,2.  '\n",
      " 'Coagulopathy, induced by Coumadin.,PLAN: , Her INR at the office was 12.  I '\n",
      " 'will repeat it, and if it is still elevated, I will give vitamin K 10 mg in '\n",
      " '100 mL of D5W and then send her home and repeat the PT/INR next week.  I '\n",
      " 'believe at this time that it is too risky to use Coumadin in her case '\n",
      " 'because of her age and comorbidities, the multiple medications that she '\n",
      " 'takes and it is very difficult to keep an adequate level of anticoagulation '\n",
      " 'that is safe for her.  She is prone to a fall and this would be a big '\n",
      " 'problem.  We will use one aspirin a day instead of the anticoagulation.  She '\n",
      " 'is aware of the risk of stroke, but she is very scared of the '\n",
      " 'anticoagulation with Coumadin and does not want to use the Coumadin at this '\n",
      " 'time and I understand.  We will see her as an outpatient.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(train_df.transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b6f4d",
   "metadata": {},
   "source": [
    "### Sample Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68977658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e142223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes = train_df[\"medical_specialty\"].unique()\n",
    "\n",
    "# idx_2_class = {i: s for i, s in enumerate(unique_classes)}\n",
    "# class_2_idx = {s: i for i, s in enumerate(unique_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac5d380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df[\"labels\"] = train_df[\"medical_specialty\"].apply(lambda s: class_2_idx[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233a4365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_train_df, train_test_df = \\\n",
    "    train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449f16bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict = {\n",
    "    'train': Dataset.from_pandas(train_train_df),\n",
    "    'val': Dataset.from_pandas(train_test_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "}\n",
    "\n",
    "ds = DatasetDict(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a995f402",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pjyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac04f8046cbd49abacb9a062cd7633be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284ef29b19cd4b1fbe6c8cab118f3682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b9ff1d4dbe452f89cdf52511657996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_text(texts):\n",
    "    texts[\"transcription\"] = [re.sub('[^A-Za-z0-9]+', ' ', string) for string in texts[\"transcription\"]]\n",
    "    stringtokens = [nltk.word_tokenize(string) for string in texts[\"transcription\"]]\n",
    "    filtered_stringtokens = [[w for w in string if not w.lower() in stop_words] for string in stringtokens]\n",
    "    texts[\"transcription\"] = [\" \".join(string) for string in filtered_stringtokens]\n",
    "    return tokenizer(texts[\"transcription\"], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(tokenize_text, batched=True)\n",
    "ds[\"val\"] = ds[\"val\"].map(tokenize_text, batched=True)\n",
    "ds[\"test\"] = ds[\"test\"].map(tokenize_text, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93ee3ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__index_level_0__': 1242,\n",
      " 'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'input_ids': [101,\n",
      "               8853,\n",
      "               2864,\n",
      "               1015,\n",
      "               20315,\n",
      "               13626,\n",
      "               4568,\n",
      "               6393,\n",
      "               8571,\n",
      "               1016,\n",
      "               23851,\n",
      "               26261,\n",
      "               22943,\n",
      "               3449,\n",
      "               20807,\n",
      "               11224,\n",
      "               2157,\n",
      "               2012,\n",
      "               14482,\n",
      "               2599,\n",
      "               1017,\n",
      "               23851,\n",
      "               26261,\n",
      "               22943,\n",
      "               3449,\n",
      "               20807,\n",
      "               11224,\n",
      "               2157,\n",
      "               18834,\n",
      "               7277,\n",
      "               7934,\n",
      "               29197,\n",
      "               2599,\n",
      "               1018,\n",
      "               8187,\n",
      "               13103,\n",
      "               23851,\n",
      "               2944,\n",
      "               13201,\n",
      "               2609,\n",
      "               2187,\n",
      "               4942,\n",
      "               20464,\n",
      "               21654,\n",
      "               12818,\n",
      "               3229,\n",
      "               12407,\n",
      "               5776,\n",
      "               6421,\n",
      "               2095,\n",
      "               2214,\n",
      "               3060,\n",
      "               2137,\n",
      "               2931,\n",
      "               25353,\n",
      "               27718,\n",
      "               9626,\n",
      "               4588,\n",
      "               10184,\n",
      "               11522,\n",
      "               2401,\n",
      "               10381,\n",
      "               4948,\n",
      "               4140,\n",
      "               18981,\n",
      "               2594,\n",
      "               4297,\n",
      "               25377,\n",
      "               12870,\n",
      "               5897,\n",
      "               28667,\n",
      "               29264,\n",
      "               2540,\n",
      "               4945,\n",
      "               8030,\n",
      "               1044,\n",
      "               22571,\n",
      "               25918,\n",
      "               20523,\n",
      "               2465,\n",
      "               23409,\n",
      "               12407,\n",
      "               4568,\n",
      "               6393,\n",
      "               8571,\n",
      "               2004,\n",
      "               17119,\n",
      "               28055,\n",
      "               12763,\n",
      "               3904,\n",
      "               4358,\n",
      "               2668,\n",
      "               3279,\n",
      "               10124,\n",
      "               10831,\n",
      "               6666,\n",
      "               15955,\n",
      "               7709,\n",
      "               4541,\n",
      "               6987,\n",
      "               5776,\n",
      "               5776,\n",
      "               2155,\n",
      "               3091,\n",
      "               9619,\n",
      "               2098,\n",
      "               7709,\n",
      "               9619,\n",
      "               2772,\n",
      "               2872,\n",
      "               3673,\n",
      "               7709,\n",
      "               5776,\n",
      "               2579,\n",
      "               15050,\n",
      "               4937,\n",
      "               2232,\n",
      "               6845,\n",
      "               17785,\n",
      "               2802,\n",
      "               7709,\n",
      "               2181,\n",
      "               2187,\n",
      "               21877,\n",
      "               16761,\n",
      "               2389,\n",
      "               3972,\n",
      "               3406,\n",
      "               3593,\n",
      "               4942,\n",
      "               20464,\n",
      "               21654,\n",
      "               2181,\n",
      "               25403,\n",
      "               2135,\n",
      "               17463,\n",
      "               5669,\n",
      "               15098,\n",
      "               5156,\n",
      "               5450,\n",
      "               2036,\n",
      "               27820,\n",
      "               3155,\n",
      "               2809,\n",
      "               2781,\n",
      "               2478,\n",
      "               11876,\n",
      "               24755,\n",
      "               3170,\n",
      "               4958,\n",
      "               3170,\n",
      "               8458,\n",
      "               11467,\n",
      "               2181,\n",
      "               2187,\n",
      "               21877,\n",
      "               16761,\n",
      "               2389,\n",
      "               3972,\n",
      "               3406,\n",
      "               3593,\n",
      "               2555,\n",
      "               4942,\n",
      "               20464,\n",
      "               21654,\n",
      "               2181,\n",
      "               3929,\n",
      "               2019,\n",
      "               4355,\n",
      "               27065,\n",
      "               3550,\n",
      "               2478,\n",
      "               2324,\n",
      "               7633,\n",
      "               5660,\n",
      "               12201,\n",
      "               2187,\n",
      "               4942,\n",
      "               20464,\n",
      "               21654,\n",
      "               12818,\n",
      "               2064,\n",
      "               11231,\n",
      "               13776,\n",
      "               2048,\n",
      "               3584,\n",
      "               4573,\n",
      "               2302,\n",
      "               7669,\n",
      "               2048,\n",
      "               3584,\n",
      "               5009,\n",
      "               20357,\n",
      "               2015,\n",
      "               12889,\n",
      "               2187,\n",
      "               4942,\n",
      "               20464,\n",
      "               21654,\n",
      "               12818,\n",
      "               5660,\n",
      "               17044,\n",
      "               3718,\n",
      "               5009,\n",
      "               20357,\n",
      "               2015,\n",
      "               7119,\n",
      "               2173,\n",
      "               19610,\n",
      "               28696,\n",
      "               2102,\n",
      "               2478,\n",
      "               2184,\n",
      "               2321,\n",
      "               21065,\n",
      "               2884,\n",
      "               6085,\n",
      "               1019,\n",
      "               4642,\n",
      "               9876,\n",
      "               4297,\n",
      "               19969,\n",
      "               2081,\n",
      "               2187,\n",
      "               21877,\n",
      "               6593,\n",
      "               10244,\n",
      "               23223,\n",
      "               3593,\n",
      "               14100,\n",
      "               3096,\n",
      "               4487,\n",
      "               11393,\n",
      "               10985,\n",
      "               14969,\n",
      "               2098,\n",
      "               21877,\n",
      "               16761,\n",
      "               13911,\n",
      "               2350,\n",
      "               6740,\n",
      "               3096,\n",
      "               25174,\n",
      "               2094,\n",
      "               2437,\n",
      "               4979,\n",
      "               13103,\n",
      "               5009,\n",
      "               20357,\n",
      "               2015,\n",
      "               5234,\n",
      "               2098,\n",
      "               2864,\n",
      "               4979,\n",
      "               3525,\n",
      "               102],\n",
      " 'labels': 7,\n",
      " 'medical_specialty': ' Cardiovascular / Pulmonary',\n",
      " 'transcription': 'PROCEDURES PERFORMED 1 DDDR permanent pacemaker 2 Insertion '\n",
      "                  'steroid eluting screw right atrial lead 3 Insertion steroid '\n",
      "                  'eluting screw right ventricular apical lead 4 Pulse '\n",
      "                  'generator insertion model Sigma SITE Left subclavian vein '\n",
      "                  'access INDICATION patient 73 year old African American '\n",
      "                  'female symptomatic bradycardia chronotropic incompetence '\n",
      "                  'recurrent heart failure symptoms hypoperfusion Class 2a '\n",
      "                  'indication permanent pacemaker ascertained COMPLICATIONS '\n",
      "                  'None ESTIMATED BLOOD LOSS Minimal Risks benefits '\n",
      "                  'alternatives procedure explained detail patient patient '\n",
      "                  'family length consented procedure consent signed placed '\n",
      "                  'chart PROCEDURE patient taken cardiac cath lab monitored '\n",
      "                  'throughout procedure area left pectoral deltoid subclavian '\n",
      "                  'area sterilely prepped draped usual manner also scrubbed '\n",
      "                  'approximately eight minutes Using lidocaine epinephrine '\n",
      "                  'area left pectoral deltoid region subclavian area fully '\n",
      "                  'anesthetized Using 18 gauge Cook needle left subclavian '\n",
      "                  'vein cannulated two separate sites without difficulty two '\n",
      "                  'separate guidewires inserted left subclavian vein Cook '\n",
      "                  'needles removed guidewires secured place hemostat Using 10 '\n",
      "                  '15 scalpel blade 5 cm horizontal incision made left '\n",
      "                  'pectodeltoid groove skin dissected blunted pectoralis major '\n",
      "                  'muscle skin undermined making pocket generator guidewires '\n",
      "                  'tunneled performed pocket Subsequently atrial ventricular '\n",
      "                  'leads inserted one Cordis separately respectively Initially '\n",
      "                  'ventricular lead inserted Cordis sheath placed guidewire '\n",
      "                  'removed thresholds appropriate position obtained '\n",
      "                  'ventricular lead Cordis sheath inserted atrial lead atrial '\n",
      "                  'lead inserted appropriately placed thresholds obtained '\n",
      "                  'Cordis removed leads sutured place pectoralis major muscle '\n",
      "                  '1 0 silk suture leads connected pulse generator pocket '\n",
      "                  'irrigated cleansed leads generators inserted pocket '\n",
      "                  'subcutaneous tissue closed gut sutures skin closed 4 0 '\n",
      "                  'polychrome sutures using subcuticular uninterrupted '\n",
      "                  'technique area cleansed dry Steri Strips pressure dressing '\n",
      "                  'applied patient tolerated procedure well complications '\n",
      "                  'Information pacemaker implanted device follows PULSE '\n",
      "                  'GENERATOR Model Name Sigma Model SDR203 Serial 123456 '\n",
      "                  'ATRIAL LEAD Model 4568 45 cm Serial 123456 RIGHT '\n",
      "                  'VENTRICULAR APICAL STEROID eluting SCREW LEAD Model 4068 52 '\n",
      "                  'cm Serial 123456 STIMULATION THRESHOLDS FOLLOWS right '\n",
      "                  'atrial chamber polarity bipolar pulse width 0 50 '\n",
      "                  'milliseconds 1 5 volts voltage 3 7 milliamps current 557 '\n",
      "                  'ohms impedance P wave sensing 3 3 millivolts right '\n",
      "                  'ventricular polarity bipolar pulse width 0 50 milliseconds '\n",
      "                  '0 7 volts voltage 1 4 milliamps current impedance 700 ohms '\n",
      "                  'R wave sensing 14 millivolts brady parameter settings set '\n",
      "                  'follows atrial ventricular appendages set 3 5 volts 0 4 '\n",
      "                  'milliseconds pulse width atrial sensitivity 0 5 180 '\n",
      "                  'milliseconds blanking Ventricular sensitivity set 2 8 28 '\n",
      "                  'milliseconds blanking pacing mode DDDR mode switch lower '\n",
      "                  'rate 70 upper rate 130 patient tolerated procedure well '\n",
      "                  'complications patient went Recovery satisfactory condition '\n",
      "                  'Family updated Orders chart Please see orders thank '\n",
      "                  'allowing participate care'}\n"
     ]
    }
   ],
   "source": [
    "pprint(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa6c4319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee028538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ca876",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f26fe53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "414f5205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "logging_steps = len(train_train_df) // batch_size\n",
    "output_dir = \"hf_trainer\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "     num_train_epochs=10,\n",
    "     learning_rate=1e-4,\n",
    "     per_device_train_batch_size=batch_size,\n",
    "     per_device_eval_batch_size=batch_size,\n",
    "     weight_decay=0.01,\n",
    "     evaluation_strategy=\"epoch\",\n",
    "     logging_steps=logging_steps,\n",
    "     push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "994876be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['val'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8cfca94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='536' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [536/870 03:47 < 02:21, 2.35 it/s, Epoch 6.15/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.973400</td>\n",
       "      <td>2.416113</td>\n",
       "      <td>0.048993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.900600</td>\n",
       "      <td>2.385623</td>\n",
       "      <td>0.053262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.861100</td>\n",
       "      <td>2.384580</td>\n",
       "      <td>0.057634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.831800</td>\n",
       "      <td>2.378683</td>\n",
       "      <td>0.060724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.759000</td>\n",
       "      <td>2.364768</td>\n",
       "      <td>0.066720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.708200</td>\n",
       "      <td>2.377490</td>\n",
       "      <td>0.056545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/transformers/trainer.py:2557\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2555\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2556\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2557\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2559\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6603a",
   "metadata": {},
   "source": [
    "### Making Inference on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc4a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1f4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_y = trainer.predict(ds[\"test\"])\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411c232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.Series(pred_y.predictions.argmax(axis=1))\n",
    "a.name = \"Expected\"\n",
    "a.to_csv(\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
